{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ReAct Agent: Deep Dive Analysis\n\n**Professional Analysis of Reasoning and Acting Framework**\n\nThis notebook provides a comprehensive analysis of the ReAct (Reasoning + Acting) framework implementation with:\n- Visual representation of agent execution graphs\n- Step-by-step reasoning trace analysis\n- Performance metrics and comparisons\n- Critical evaluation of tool usage patterns\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport sys\nfrom pathlib import Path\nimport json\nimport time\nfrom datetime import datetime\n\n# Add project to path\nsys.path.insert(0, str(Path.cwd().parent))\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport networkx as nx\nfrom IPython.display import display, HTML, Image, Markdown\n\n# Project imports\nfrom src.agents.agent_factory import AgentFactory\nfrom src.agents.agent_executor import AgentExecutor\nfrom src.database.db_manager import DatabaseManager\nfrom src.config import config\n\n# Styling\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\nprint(\"All imports loaded successfully\")\nprint(f\"OpenAI API Key configured: {bool(config.openai_api_key)}\")\nprint(f\"Database path: {config.database_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ReAct Framework Overview\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "ReAct combines **Reasoning** (thinking about what to do) and **Acting** (using tools to do it) in a unified framework.\n",
    "\n",
    "**Key Insight**: Instead of just generating text, the LLM can reason about which tools to use and when, creating a transparent decision-making process.\n",
    "\n",
    "### ReAct Loop:\n",
    "\n",
    "```\n",
    "Query → [Thought → Action → Observation]* → Final Answer\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Thought**: LLM reasons about what to do next\n",
    "- **Action**: LLM selects a tool and provides input\n",
    "- **Observation**: Tool returns results to LLM\n",
    "- **Repeat** until LLM has enough information to answer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Agent and Tools\n",
    "\n",
    "We'll test with two different query types to understand how ReAct adapts:\n",
    "1. **Database Query**: Requires searching the Pink Floyd database\n",
    "2. **Currency Query**: Requires calling external currency API\n",
    "3. **Multi-Tool Query**: Requires using both tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize agent factory\nfactory = AgentFactory()\n\n# Create agent with gpt-4o-mini (fast and cost-effective)\nmodel_name = \"gpt-4o-mini\"\nagent = factory.create_agent(model_name)\nexecutor = AgentExecutor(agent, model_name)\n\n# Display available tools\nprint(\"Available Tools:\")\nprint(\"=\"*60)\nfor i, tool in enumerate(factory.get_tools(), 1):\n    print(f\"{i}. {tool.name}\")\n    print(f\"   Description: {tool.description[:100]}...\")\n    print()\n\nprint(f\"Agent initialized with model: {model_name}\")\nprint(f\"{len(factory.get_tools())} tools available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Case 1: Database Query\n",
    "\n",
    "**Query**: \"Find melancholic Pink Floyd songs from the 1970s\"\n",
    "\n",
    "**Expected Behavior**:\n",
    "- Agent should recognize this requires database search\n",
    "- Should use `pink_floyd_database` tool\n",
    "- Should parse mood=\"melancholic\" and decade=\"1970s\"\n",
    "- Should format results clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute database query\nquery1 = \"Find melancholic Pink Floyd songs from the 1970s\"\n\nprint(f\"Query: {query1}\")\nprint(\"=\"*60)\nprint(\"Executing...\\n\")\n\nresult1 = executor.execute(query1)\n\n# Display answer\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANSWER:\")\nprint(\"=\"*60)\nprint(result1[\"answer\"])\nprint(\"\\n\" + \"=\"*60)\n\n# Display metrics\nmetrics = result1[\"metrics\"]\nprint(f\"\\nExecution Time: {metrics['execution_time_seconds']}s\")\nprint(f\"Tokens Used: {metrics['estimated_tokens']['total']}\")\nprint(f\"Estimated Cost: ${metrics['estimated_cost_usd']:.6f}\")\nprint(f\"Reasoning Steps: {metrics['num_steps']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize Reasoning Trace - Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reasoning_trace(trace, title=\"ReAct Execution Graph\"):\n",
    "    \"\"\"\n",
    "    Create a visual representation of the ReAct reasoning trace.\n",
    "    \n",
    "    Uses NetworkX to build a directed graph showing:\n",
    "    - Query node (starting point)\n",
    "    - Thought nodes (reasoning)\n",
    "    - Action nodes (tool calls)\n",
    "    - Observation nodes (tool results)\n",
    "    - Answer node (final output)\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Color mapping for different node types\n",
    "    colors = {\n",
    "        'query': '#FF1493',      # Pink\n",
    "        'thought': '#4169E1',    # Blue\n",
    "        'action': '#32CD32',     # Green\n",
    "        'observation': '#FFA500',# Orange\n",
    "        'answer': '#9370DB'      # Purple\n",
    "    }\n",
    "    \n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    labels = {}\n",
    "    \n",
    "    # Add query node\n",
    "    G.add_node(0)\n",
    "    node_colors.append(colors['query'])\n",
    "    node_sizes.append(3000)\n",
    "    labels[0] = \"Query\"\n",
    "    \n",
    "    prev_node = 0\n",
    "    node_id = 1\n",
    "    \n",
    "    # Process trace steps\n",
    "    for step in trace:\n",
    "        step_type = step.get('type', '')\n",
    "        \n",
    "        if step_type in ['thought', 'action', 'observation']:\n",
    "            G.add_node(node_id)\n",
    "            G.add_edge(prev_node, node_id)\n",
    "            \n",
    "            # Set color and label based on type\n",
    "            if step_type == 'action':\n",
    "                node_colors.append(colors['action'])\n",
    "                node_sizes.append(2500)\n",
    "                tool = step.get('tool', 'unknown')\n",
    "                labels[node_id] = f\"Action\\n{tool}\"\n",
    "            elif step_type == 'observation':\n",
    "                node_colors.append(colors['observation'])\n",
    "                node_sizes.append(2500)\n",
    "                labels[node_id] = \"Observation\"\n",
    "            else:\n",
    "                node_colors.append(colors['thought'])\n",
    "                node_sizes.append(2000)\n",
    "                labels[node_id] = \"Thought\"\n",
    "            \n",
    "            prev_node = node_id\n",
    "            node_id += 1\n",
    "    \n",
    "    # Add answer node\n",
    "    G.add_node(node_id)\n",
    "    G.add_edge(prev_node, node_id)\n",
    "    node_colors.append(colors['answer'])\n",
    "    node_sizes.append(3000)\n",
    "    labels[node_id] = \"Answer\"\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Use hierarchical layout for better readability\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Draw graph\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_color=node_colors,\n",
    "                          node_size=node_sizes,\n",
    "                          alpha=0.9)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos,\n",
    "                          edge_color='gray',\n",
    "                          arrows=True,\n",
    "                          arrowsize=20,\n",
    "                          arrowstyle='->',\n",
    "                          width=2)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels,\n",
    "                           font_size=9,\n",
    "                           font_weight='bold')\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['query'], markersize=10, label='Query'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['thought'], markersize=10, label='Thought'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['action'], markersize=10, label='Action'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['observation'], markersize=10, label='Observation'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['answer'], markersize=10, label='Answer')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Visualize the database query execution\n",
    "print(\"Database Query Execution Graph:\")\n",
    "print(\"=\"*60)\n",
    "graph1 = visualize_reasoning_trace(\n",
    "    result1[\"reasoning_trace\"],\n",
    "    title=\"ReAct: Database Query Execution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Detailed Trace Analysis - Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_trace(trace, query):\n    \"\"\"\n    Analyze reasoning trace with critical evaluation.\n    \"\"\"\n    print(f\"Query: {query}\")\n    print(\"=\"*80)\n    print()\n    \n    action_count = 0\n    tools_used = set()\n    \n    for i, step in enumerate(trace, 1):\n        step_type = step.get('type', '')\n        \n        if step_type == 'query':\n            print(f\"[QUERY] STEP {i}: USER QUERY\")\n            print(f\"   Content: {step.get('content', '')[:100]}...\")\n            \n        elif step_type == 'thought':\n            print(f\"\\n[THOUGHT] STEP {i}: THOUGHT (Reasoning)\")\n            print(f\"   {step.get('content', '')[:200]}...\")\n            \n        elif step_type == 'action':\n            action_count += 1\n            tool = step.get('tool', 'unknown')\n            tools_used.add(tool)\n            print(f\"\\n[ACTION] STEP {i}: ACTION (Tool Call #{action_count})\")\n            print(f\"   Tool: {tool}\")\n            print(f\"   Input: {json.dumps(step.get('input', {}), indent=6)}\")\n            \n        elif step_type == 'observation':\n            print(f\"\\n[OBSERVATION] STEP {i}: OBSERVATION (Tool Result)\")\n            content = step.get('content', '')\n            if len(content) > 300:\n                print(f\"   Result: {content[:300]}...\")\n                print(f\"   (truncated - full length: {len(content)} chars)\")\n            else:\n                print(f\"   Result: {content}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY:\")\n    print(f\"  Total Steps: {len(trace)}\")\n    print(f\"  Tool Calls: {action_count}\")\n    print(f\"  Tools Used: {', '.join(tools_used)}\")\n    print(\"=\"*80)\n\nanalyze_trace(result1[\"reasoning_trace\"], query1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Critical Analysis - Database Query\n",
    "\n",
    "**What went well:**\n",
    "- Agent correctly identified need for database tool\n",
    "- Extracted relevant parameters (mood, decade)\n",
    "- Formatted results in user-friendly way\n",
    "\n",
    "**Potential improvements:**\n",
    "- Could the agent have been more specific in the query?\n",
    "- Is one tool call sufficient or should it verify results?\n",
    "- How does answer quality compare to ground truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Test Case 2: Currency Query\n",
    "\n",
    "**Query**: \"What is the current exchange rate from USD to EUR?\"\n",
    "\n",
    "**Expected Behavior**:\n",
    "- Agent should recognize this requires currency data\n",
    "- Should use `currency_price_checker` tool\n",
    "- Should parse currency pair (USD/EUR)\n",
    "- Should provide current rate with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute currency query\nquery2 = \"What is the current exchange rate from USD to EUR?\"\n\nprint(f\"Query: {query2}\")\nprint(\"=\"*60)\nprint(\"Executing...\\n\")\n\nresult2 = executor.execute(query2)\n\n# Display answer\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANSWER:\")\nprint(\"=\"*60)\nprint(result2[\"answer\"])\nprint(\"\\n\" + \"=\"*60)\n\n# Display metrics\nmetrics = result2[\"metrics\"]\nprint(f\"\\nExecution Time: {metrics['execution_time_seconds']}s\")\nprint(f\"Tokens Used: {metrics['estimated_tokens']['total']}\")\nprint(f\"Estimated Cost: ${metrics['estimated_cost_usd']:.6f}\")\nprint(f\"Reasoning Steps: {metrics['num_steps']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualize Reasoning Trace - Currency Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the currency query execution\n",
    "print(\"Currency Query Execution Graph:\")\n",
    "print(\"=\"*60)\n",
    "graph2 = visualize_reasoning_trace(\n",
    "    result2[\"reasoning_trace\"],\n",
    "    title=\"ReAct: Currency Query Execution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Detailed Trace Analysis - Currency Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_trace(result2[\"reasoning_trace\"], query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Test Case 3: Multi-Tool Query\n",
    "\n",
    "**Query**: \"I want energetic Pink Floyd music to listen to, and also tell me the EUR to GBP exchange rate\"\n",
    "\n",
    "**Expected Behavior**:\n",
    "- Agent must recognize TWO distinct tasks\n",
    "- Should use BOTH tools sequentially\n",
    "- Should combine results coherently\n",
    "- **This tests agent's ability to handle complex, multi-step queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute multi-tool query\nquery3 = \"I want energetic Pink Floyd music to listen to, and also tell me the EUR to GBP exchange rate\"\n\nprint(f\"Query: {query3}\")\nprint(\"=\"*60)\nprint(\"Executing...\\n\")\n\nresult3 = executor.execute(query3)\n\n# Display answer\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANSWER:\")\nprint(\"=\"*60)\nprint(result3[\"answer\"])\nprint(\"\\n\" + \"=\"*60)\n\n# Display metrics\nmetrics = result3[\"metrics\"]\nprint(f\"\\nExecution Time: {metrics['execution_time_seconds']}s\")\nprint(f\"Tokens Used: {metrics['estimated_tokens']['total']}\")\nprint(f\"Estimated Cost: ${metrics['estimated_cost_usd']:.6f}\")\nprint(f\"Reasoning Steps: {metrics['num_steps']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualize Reasoning Trace - Multi-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multi-tool query execution\n",
    "print(\"Multi-Tool Query Execution Graph:\")\n",
    "print(\"=\"*60)\n",
    "graph3 = visualize_reasoning_trace(\n",
    "    result3[\"reasoning_trace\"],\n",
    "    title=\"ReAct: Multi-Tool Query Execution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detailed Trace Analysis - Multi-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_trace(result3[\"reasoning_trace\"], query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Critical Analysis - Multi-Tool Query\n",
    "\n",
    "**Key Observations:**\n",
    "1. Did the agent correctly identify both sub-tasks?\n",
    "2. Were tools called in logical order?\n",
    "3. Was the final answer coherent and complete?\n",
    "4. How many additional reasoning steps were needed vs single-tool queries?\n",
    "\n",
    "**This demonstrates the agent's ability to:**\n",
    "- Decompose complex queries\n",
    "- Coordinate multiple tools\n",
    "- Synthesize disparate information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comparative Analysis\n",
    "\n",
    "Compare performance across all three test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics\n",
    "comparison_data = {\n",
    "    'Query Type': ['Database Only', 'Currency Only', 'Multi-Tool'],\n",
    "    'Query': [\n",
    "        query1[:40] + '...',\n",
    "        query2[:40] + '...',\n",
    "        query3[:40] + '...'\n",
    "    ],\n",
    "    'Execution Time (s)': [\n",
    "        result1['metrics']['execution_time_seconds'],\n",
    "        result2['metrics']['execution_time_seconds'],\n",
    "        result3['metrics']['execution_time_seconds']\n",
    "    ],\n",
    "    'Total Tokens': [\n",
    "        result1['metrics']['estimated_tokens']['total'],\n",
    "        result2['metrics']['estimated_tokens']['total'],\n",
    "        result3['metrics']['estimated_tokens']['total']\n",
    "    ],\n",
    "    'Cost (USD)': [\n",
    "        result1['metrics']['estimated_cost_usd'],\n",
    "        result2['metrics']['estimated_cost_usd'],\n",
    "        result3['metrics']['estimated_cost_usd']\n",
    "    ],\n",
    "    'Reasoning Steps': [\n",
    "        result1['metrics']['num_steps'],\n",
    "        result2['metrics']['num_steps'],\n",
    "        result3['metrics']['num_steps']\n",
    "    ],\n",
    "    'Tools Used': [\n",
    "        len([s for s in result1['reasoning_trace'] if s.get('type') == 'action']),\n",
    "        len([s for s in result2['reasoning_trace'] if s.get('type') == 'action']),\n",
    "        len([s for s in result3['reasoning_trace'] if s.get('type') == 'action'])\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display table\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "display(df.style.background_gradient(cmap='Blues', subset=['Execution Time (s)', 'Total Tokens', 'Reasoning Steps']))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Execution Time\n",
    "axes[0, 0].bar(df['Query Type'], df['Execution Time (s)'], color=['#FF1493', '#4169E1', '#32CD32'])\n",
    "axes[0, 0].set_title('Execution Time Comparison', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Seconds')\n",
    "axes[0, 0].set_ylim(0, max(df['Execution Time (s)']) * 1.2)\n",
    "\n",
    "# 2. Token Usage\n",
    "axes[0, 1].bar(df['Query Type'], df['Total Tokens'], color=['#FF1493', '#4169E1', '#32CD32'])\n",
    "axes[0, 1].set_title('Token Usage Comparison', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Tokens')\n",
    "\n",
    "# 3. Cost\n",
    "axes[1, 0].bar(df['Query Type'], df['Cost (USD)'], color=['#FF1493', '#4169E1', '#32CD32'])\n",
    "axes[1, 0].set_title('Cost Comparison', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('USD')\n",
    "axes[1, 0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# 4. Complexity (Steps + Tools)\n",
    "x = range(len(df))\n",
    "width = 0.35\n",
    "axes[1, 1].bar([i - width/2 for i in x], df['Reasoning Steps'], width, label='Reasoning Steps', color='#4169E1')\n",
    "axes[1, 1].bar([i + width/2 for i in x], df['Tools Used'], width, label='Tools Used', color='#32CD32')\n",
    "axes[1, 1].set_title('Complexity Comparison', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(df['Query Type'])\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Critical Insights\n",
    "\n",
    "### 7.1 ReAct Framework Strengths\n",
    "\n",
    "1. **Transparency**: Every decision is visible in the reasoning trace\n",
    "2. **Tool Selection**: Agent autonomously chooses correct tools\n",
    "3. **Adaptability**: Handles single-tool and multi-tool queries\n",
    "4. **Iterative Refinement**: Can call tools multiple times if needed\n",
    "\n",
    "### 7.2 Observed Patterns\n",
    "\n",
    "**From our experiments:**\n",
    "\n",
    "- **Database queries**: Typically 1 tool call, fast execution\n",
    "- **Currency queries**: 1 tool call, similar performance\n",
    "- **Multi-tool queries**: 2+ tool calls, proportionally more tokens/time\n",
    "\n",
    "**Complexity scaling**:\n",
    "- Linear increase in tokens with number of tools\n",
    "- Execution time grows sub-linearly (tool calls can be parallel in theory)\n",
    "- Cost scales predictably with token usage\n",
    "\n",
    "### 7.3 Potential Failure Modes\n",
    "\n",
    "1. **Hallucination**: Agent might \"make up\" tool results\n",
    "2. **Tool Selection Error**: Choosing wrong tool for task\n",
    "3. **Incomplete Parsing**: Missing parameters in tool input\n",
    "4. **Infinite Loops**: Agent keeps calling tools without progress\n",
    "5. **Context Overflow**: Too many iterations exceed context window\n",
    "\n",
    "### 7.4 Production Considerations\n",
    "\n",
    "**Must implement:**\n",
    "- Max iteration limits (prevent infinite loops)\n",
    "- Input validation (sanitize tool inputs)\n",
    "- Error handling (graceful degradation)\n",
    "- Logging (debug tool usage patterns)\n",
    "- Rate limiting (prevent API abuse)\n",
    "\n",
    "**Nice to have:**\n",
    "- Tool call caching (avoid redundant calls)\n",
    "- Parallel tool execution (speed up multi-tool queries)\n",
    "- Confidence scoring (flag uncertain answers)\n",
    "- User feedback loop (improve over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Database Verification\n",
    "\n",
    "Let's verify the database query results to check agent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check what's actually in the database\ndb_manager = DatabaseManager(config.database_path)\n\n# Get melancholic songs from 1970s\nmelancholic_songs = db_manager.get_songs_by_mood(\"melancholic\")\nsongs_1970s = [s for s in melancholic_songs if 1970 <= s.year <= 1979]\n\nprint(\"Ground Truth: Melancholic Pink Floyd Songs from 1970s\")\nprint(\"=\"*60)\nprint(f\"Found {len(songs_1970s)} songs:\\n\")\n\nfor song in songs_1970s:\n    print(f\"  - {song.title} ({song.year}) - {song.album}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nAgent Answer Accuracy Check:\")\nprint(\"Compare the agent's answer above with this ground truth.\")\nprint(\"Did the agent:\")\nprint(\"  [OK] Find all songs?\")\nprint(\"  [OK] Include only songs from 1970s?\")\nprint(\"  [OK] Include only melancholic songs?\")\nprint(\"  [OK] Format results clearly?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Model Comparison (Optional)\n",
    "\n",
    "Compare different models on the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare gpt-4o-mini vs gpt-4o\nmodels = [\"gpt-4o-mini\", \"gpt-4o\"]\ntest_query = \"Find melancholic Pink Floyd songs\"\n\nresults = {}\n\nprint(f\"Testing query: {test_query}\")\nprint(\"=\"*60)\n\nfor model in models:\n    print(f\"\\nTesting {model}...\")\n    \n    try:\n        agent = factory.create_agent(model)\n        executor = AgentExecutor(agent, model)\n        result = executor.execute(test_query)\n        \n        results[model] = {\n            'time': result['metrics']['execution_time_seconds'],\n            'tokens': result['metrics']['estimated_tokens']['total'],\n            'cost': result['metrics']['estimated_cost_usd'],\n            'steps': result['metrics']['num_steps'],\n            'answer_length': len(result['answer'])\n        }\n        \n        print(f\"  [OK] Time: {results[model]['time']}s\")\n        print(f\"  [OK] Cost: ${results[model]['cost']:.6f}\")\n        print(f\"  [OK] Steps: {results[model]['steps']}\")\n        \n    except Exception as e:\n        print(f\"  [ERROR] Error: {e}\")\n        results[model] = None\n\n# Compare results\nif all(results.values()):\n    print(\"\\n\" + \"=\"*60)\n    print(\"Model Comparison Summary:\")\n    print(\"=\"*60)\n    \n    comparison_df = pd.DataFrame(results).T\n    display(comparison_df.style.highlight_min(color='lightgreen', subset=['time', 'cost']))\n    \n    # Calculate differences\n    if len(models) == 2:\n        m1, m2 = models\n        time_diff = (results[m2]['time'] / results[m1]['time'] - 1) * 100\n        cost_diff = (results[m2]['cost'] / results[m1]['cost'] - 1) * 100\n        \n        print(f\"\\n{m2} vs {m1}:\")\n        print(f\"  Time: {time_diff:+.1f}% {'slower' if time_diff > 0 else 'faster'}\")\n        print(f\"  Cost: {cost_diff:+.1f}% {'more expensive' if cost_diff > 0 else 'cheaper'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **ReAct provides transparency**: Every decision is traceable\n",
    "2. **Tool selection is reliable**: Agent consistently chooses correct tools\n",
    "3. **Performance is predictable**: Cost and time scale linearly with complexity\n",
    "4. **Multi-tool queries work**: Agent can coordinate multiple tools effectively\n",
    "\n",
    "### When to Use ReAct\n",
    "\n",
    "**Good for:**\n",
    "- Tasks requiring external data/tools\n",
    "- Situations needing transparency/explainability\n",
    "- Multi-step reasoning workflows\n",
    "- Dynamic tool selection scenarios\n",
    "\n",
    "**Not ideal for:**\n",
    "- Pure text generation (no tools needed)\n",
    "- Latency-critical applications (multiple LLM calls)\n",
    "- Simple classification tasks (overkill)\n",
    "- When tool reliability is uncertain\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Add more tools**: Expand agent capabilities\n",
    "2. **Implement caching**: Reduce redundant tool calls\n",
    "3. **Add validation**: Verify tool results before using\n",
    "4. **Monitor in production**: Track failure modes\n",
    "5. **A/B test models**: Find optimal cost/performance balance\n",
    "\n",
    "---\n",
    "\n",
    "**Further Reading:**\n",
    "- ReAct Paper: https://arxiv.org/abs/2210.03629\n",
    "- LangChain Agents: https://python.langchain.com/docs/modules/agents/\n",
    "- LangGraph: https://langchain-ai.github.io/langgraph/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}